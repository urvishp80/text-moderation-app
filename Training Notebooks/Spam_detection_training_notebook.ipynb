{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"../requirements.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILEPATH = r\"Text Moderation\\Dataset\\messages.csv\" # Path to training Data Csv file\n",
    "INPUT_DATA_COLUMN_NAME = \"C3\" # Represent 3rd Column\n",
    "TARGET_DATA_COLUMN_NAME = \"C5\" # Represent 5th Column\n",
    "MAX_COLUMN_TO_READ = 100 # Default is 100 and working fine\n",
    "MODEL_WEIGHT_PATH = r\"../weights/spam_model_weights/Spam_Model_weight\" # Default path to save weights\n",
    "\n",
    "\n",
    "#Model training parameters\n",
    "NUM_OF_DATASET_SPLIT = 5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA operations library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text preprocessing library\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Deep learning model library\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Function\n",
    "\n",
    "1. convert_to_number(pd.Series:): Convert the String series to number with Error handling\n",
    "2. remove_html_tags_special_character(pd.Series): Remove all unnecessary Html tags from the input data\n",
    "3. remove_punctuations (pd.Series): Remove all punctuation from input data\n",
    "4. remove_stopwords (text: str): remove all english stop words from input text.\n",
    "5. balance_data(df: pd.DataFrame, y_column_name: target Column Name): balance out the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_number(series):\n",
    "    for i  in range(len(series)):\n",
    "        try:\n",
    "            series[i] = int(series[i])\n",
    "        except ValueError:\n",
    "            series[i] = None\n",
    "    return series\n",
    "\n",
    "\n",
    "def remove_html_tags_special_character(col: pd.Series) -> pd.Series:\n",
    "    tags_list = ['<p>' ,'</p>' , '<p*>',\n",
    "                 '<ul>','</ul>',\n",
    "                 '<li>','</li>',\n",
    "                 '<br>',\n",
    "                 '<strong>','</strong>',\n",
    "                 '<span*>','</span>',\n",
    "                 '<a href*>','</a>',\n",
    "                 '<em>','</em>','<br>','<br />','<div>','</div>','\\\\n','~']\n",
    "    for tag in tags_list:\n",
    "        col.replace(to_replace=tag,value='',regex=False,inplace=True)\n",
    "    return col\n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    temp = str.maketrans('', '', punctuations_list)\n",
    "    text = str(text)\n",
    "    return text.translate(temp)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    "        word = word.lower()\n",
    " \n",
    "        if (word not in stop_words) and 'br' not in word:\n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output\n",
    "\n",
    "def balance_data(df,y_column_name):\n",
    "    ham_msg = df[df[y_column_name] == 0]\n",
    "    spam_msg = df[df[y_column_name] == 1]\n",
    "    if len(ham_msg) >= len(spam_msg):\n",
    "        ham_msg = ham_msg.sample(n=len(spam_msg),random_state=42)\n",
    "    else:\n",
    "        spam_msg = spam_msg.sample(n=len(ham_msg),random_state=42)\n",
    "    return pd.concat([ham_msg, spam_msg],ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess function to clean the dataset for model Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,X_column_name=\"C3\",y_column_name=\"C5\",split_ratio=[0.1,0.2]):\n",
    "    \"\"\"\n",
    "    Preprocess the data for model training\n",
    "    Arguments:\n",
    "    df: Dataframe of raw data\n",
    "    X_column_name: represent the input x column name in DataFrame\n",
    "    Y_Column_nameL represent the target y column name in DataFrame\n",
    "    split_ratio: use to define spliting ratio for training and testing data default is 0.2 (20% of data is use for testing and 80% for training)\n",
    "    \"\"\"\n",
    "    # Target value preprocessing\n",
    "    df = df[[X_column_name,y_column_name]] # C3 for input column ,C5 target column\n",
    "    df[y_column_name] = pd.Series(convert_to_number(df[y_column_name].to_list()),name=y_column_name)\n",
    "    df = df[ df[y_column_name] <= 1]\n",
    "\n",
    "    #input Value preprocessing\n",
    "    #Step 1 remove html Tags an extra special character\n",
    "    df[X_column_name] = remove_html_tags_special_character(df[X_column_name])\n",
    "    df[X_column_name].replace(to_replace='\\n',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].replace(to_replace='\\\\?',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].dropna(inplace=True)\n",
    "    \n",
    "    # Step 3 NLP Text Preprocessing\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda x: remove_punctuations(x))\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda text: remove_stopwords(text))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing raw Data\n",
    "raw_df = pd.read_csv(TRAINING_FILEPATH, encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(MAX_COLUMN_TO_READ)])\n",
    "df = preprocess(raw_df,\n",
    "                INPUT_DATA_COLUMN_NAME,\n",
    "                TARGET_DATA_COLUMN_NAME)\n",
    "df.reset_index(inplace=True)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=df[TARGET_DATA_COLUMN_NAME].unique(),\n",
    "    y= df[TARGET_DATA_COLUMN_NAME]\n",
    "    )\n",
    "class_weights = {0:class_weights[0], 1:class_weights[1]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding using BERT uncase encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode(texts, tokenizer, max_len):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_OF_DATASET_SPLIT,shuffle=True)\n",
    "MAX_LENGTH = 100\n",
    "# Load the pre-trained model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in skf.split(df[INPUT_DATA_COLUMN_NAME], df[TARGET_DATA_COLUMN_NAME]):\n",
    "    X_train, X_val = df[INPUT_DATA_COLUMN_NAME][train_idx], df[INPUT_DATA_COLUMN_NAME][val_idx]\n",
    "    y_train, y_val = df[TARGET_DATA_COLUMN_NAME][train_idx], df[TARGET_DATA_COLUMN_NAME][val_idx]\n",
    "\n",
    "    train_input_ids, train_attention_mask = encode(X_train, tokenizer, MAX_LENGTH)\n",
    "    val_input_ids, val_attention_mask = encode(X_val, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    EPOCHS = EPOCHS\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        [train_input_ids, train_attention_mask],\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=([val_input_ids, val_attention_mask], y_val),\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    preds = model.predict([val_input_ids, val_attention_mask])\n",
    "    preds = np.argmax(preds[0], axis=1)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, preds, average=\"binary\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Model Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(MODEL_WEIGHT_PATH)\n",
    "print(f\"Spam Model weights are saved at {MODEL_WEIGHT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
