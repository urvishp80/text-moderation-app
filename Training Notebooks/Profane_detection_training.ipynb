{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\codej\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Function For Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Function\n",
    "\n",
    "def convert_to_number(series):\n",
    "    for i  in range(len(series)):\n",
    "        try:\n",
    "            series[i] = int(series[i])\n",
    "        except ValueError:\n",
    "            series[i] = None\n",
    "    return series\n",
    "\n",
    "\n",
    "def remove_html_tags_special_character(col: pd.Series) -> pd.Series:\n",
    "    tags_list = ['<p>' ,'</p>' , '<p*>',\n",
    "                 '<ul>','</ul>',\n",
    "                 '<li>','</li>',\n",
    "                 '<br>',\n",
    "                 '<strong>','</strong>',\n",
    "                 '<span*>','</span>',\n",
    "                 '<a href*>','</a>',\n",
    "                 '<em>','</em>','<br>','<br />','<div>','</div>','\\\\n','~']\n",
    "    for tag in tags_list:\n",
    "        col.replace(to_replace=tag,value='',regex=False,inplace=True)\n",
    "    return col\n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    temp = str.maketrans('', '', punctuations_list)\n",
    "    text = str(text)\n",
    "    return text.translate(temp)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    "        word = word.lower()\n",
    " \n",
    "        if (word not in stop_words) and 'br' not in word:\n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output\n",
    "\n",
    "def balance_data(df,y_column_name):\n",
    "    ham_msg = df[df[y_column_name] == 0]\n",
    "    spam_msg = df[df[y_column_name] == 1]\n",
    "    print(ham_msg.shape)\n",
    "    print(spam_msg.shape)\n",
    "\n",
    "    if len(ham_msg) >= len(spam_msg):\n",
    "        ham_msg.sample(n=len(spam_msg),random_state=42)\n",
    "    else:\n",
    "        spam_msg.sample(n=len(ham_msg),random_state=42)\n",
    "    return pd.concat([ham_msg, spam_msg],ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprcessor Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,X_column_name: str,y_column_name: str,split_ratio = [0.1,0.2]):\n",
    "    \"\"\"\n",
    "    Preprocess the data for model training\n",
    "    Arguments:\n",
    "    df: Dataframe of raw data\n",
    "    X_column_name: represent the input x column name in DataFrame\n",
    "    Y_Column_nameL represent the target y column name in DataFrame\n",
    "    split_ratio: use to define spliting ratio for training and testing data default is 0.2 (20% of data is use for testing and 80% for training)\n",
    "    \"\"\"\n",
    "    # Target value preprocessing\n",
    "    df = df[[X_column_name,y_column_name]] # C3 for input column ,C5 target column\n",
    "    df[y_column_name] = pd.Series(convert_to_number(df[y_column_name].to_list()),name=y_column_name)\n",
    "    df = df[ df[y_column_name] <= 1]\n",
    "\n",
    "    #input Value preprocessing\n",
    "    #Step 1 remove html Tags an extra special character\n",
    "    df[X_column_name] = remove_html_tags_special_character(df[X_column_name])\n",
    "    df[X_column_name].replace(to_replace='\\n',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].replace(to_replace='\\\\?',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].dropna(inplace=True)\n",
    "\n",
    "    # Balance Data\n",
    "    df = balance_data(df,y_column_name)\n",
    "    # Step 3 NLP Text Preprocessing\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda x: remove_punctuations(x))\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda text: remove_stopwords(text))\n",
    "    \n",
    "    if len(split_ratio) == 2:\n",
    "        train_x, test_x, train_y, test_y = train_test_split(df[X_column_name],df[y_column_name],test_size=split_ratio[1])\n",
    "        train_x, val_x, train_y, val_y = train_test_split(train_x,train_y,test_size=split_ratio[0])\n",
    "        return ((train_x,train_y), (val_x, val_y), (test_x, test_y))\n",
    "    else:\n",
    "        train_x, test_x, train_y, test_y = train_test_split(df[X_column_name],df[y_column_name],test_size=split_ratio[0])\n",
    "        return ((train_x, train_y), (test_x, test_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Set the training dataset path\n",
    " 2. Set the input colum name and target column name (all column name updated as C1, C2, C3 so if  input column is 3 so set  INPUT_COLUMN_NAME = C3 same for TARGET_COLUMN_NAME)\n",
    " 3. set the training testing and validation ratio. i.e ([0.1,0.2], 10% data will use for validation, 20% data will use for testing and 70% data will use for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codej\\AppData\\Local\\Temp\\ipykernel_6444\\3981657154.py:5: DtypeWarning: Columns (9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATASET_PATH,encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(Max_COLUMN)])\n",
      "C:\\Users\\codej\\AppData\\Local\\Temp\\ipykernel_6444\\2625709502.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[y_column_name] = pd.Series(convert_to_number(df[y_column_name].to_list()),name=y_column_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578489, 2)\n",
      "(91143, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\codej\\OneDrive\\Documents\\CJ_online_work\\text-moderation-app\\Training Notebooks\\Profane_detection_training.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m VALIDATION_RATIO \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m TESTING_RATIO \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_set, val_set, test_set \u001b[39m=\u001b[39m preprocess(df,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                           INPUT_COLUMN_NAME,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                           TARGET_COLUMN_NAME,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                           [VALIDATION_RATIO,TESTING_RATIO])\n",
      "\u001b[1;32mc:\\Users\\codej\\OneDrive\\Documents\\CJ_online_work\\text-moderation-app\\Training Notebooks\\Profane_detection_training.ipynb Cell 9\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(df, X_column_name, y_column_name, split_ratio)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Step 3 NLP Text Preprocessing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df[X_column_name] \u001b[39m=\u001b[39m df[X_column_name]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: remove_punctuations(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df[X_column_name] \u001b[39m=\u001b[39m df[X_column_name]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m text: remove_stopwords(text))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(split_ratio) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     train_x, test_x, train_y, test_y \u001b[39m=\u001b[39m train_test_split(df[X_column_name],df[y_column_name],test_size\u001b[39m=\u001b[39msplit_ratio[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\codej\\.conda\\envs\\Kaggle_ML\\lib\\site-packages\\pandas\\core\\series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4517\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4521\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4522\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4523\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4524\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4525\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4624\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4625\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4626\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\codej\\.conda\\envs\\Kaggle_ML\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\codej\\.conda\\envs\\Kaggle_ML\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\codej\\.conda\\envs\\Kaggle_ML\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\codej\\OneDrive\\Documents\\CJ_online_work\\text-moderation-app\\Training Notebooks\\Profane_detection_training.ipynb Cell 9\u001b[0m in \u001b[0;36mpreprocess.<locals>.<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Step 3 NLP Text Preprocessing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df[X_column_name] \u001b[39m=\u001b[39m df[X_column_name]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: remove_punctuations(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df[X_column_name] \u001b[39m=\u001b[39m df[X_column_name]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: remove_stopwords(text))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(split_ratio) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     train_x, test_x, train_y, test_y \u001b[39m=\u001b[39m train_test_split(df[X_column_name],df[y_column_name],test_size\u001b[39m=\u001b[39msplit_ratio[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\codej\\OneDrive\\Documents\\CJ_online_work\\text-moderation-app\\Training Notebooks\\Profane_detection_training.ipynb Cell 9\u001b[0m in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_stopwords\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     stop_words \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     imp_words \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codej/OneDrive/Documents/CJ_online_work/text-moderation-app/Training%20Notebooks/Profane_detection_training.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# Storing the important words\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[39m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(_path):\n\u001b[0;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "File \u001b[1;32mc:\\Users\\codej\\.conda\\envs\\Kaggle_ML\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATASET_PATH = r\"A:\\CJ_Personal\\Upwork\\Text Moderation\\Dataset\\messages.csv\"\n",
    "INPUT_COLUMN_NAME = \"C3\"\n",
    "TARGET_COLUMN_NAME = \"C5\"\n",
    "Max_COLUMN = 100\n",
    "df = pd.read_csv(DATASET_PATH,encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(Max_COLUMN)])\n",
    "VALIDATION_RATIO = 0.1\n",
    "TESTING_RATIO = 0.2\n",
    "train_set, val_set, test_set = preprocess(df,\n",
    "                                          INPUT_COLUMN_NAME,\n",
    "                                          TARGET_COLUMN_NAME,\n",
    "                                          [VALIDATION_RATIO,TESTING_RATIO])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Pre-build Bert Model Layer\n",
    "## Variants of bert (https://tfhub.dev/google/collections/bert/1)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "bert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model Wrapper ( input and output layer added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert model wth input and output layer wrapper\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape = (), dtype = tf.string, name = 'Inputs')\n",
    "preprocessed_text = bert_preprocessor(text_input)\n",
    "embeed = bert_encoder(preprocessed_text)\n",
    "dropout = tf.keras.layers.Dropout(0.1, name = 'Dropout')(embeed['pooled_output'])\n",
    "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'Dense')(dropout)\n",
    "# creating final model\n",
    "model = tf.keras.Model(inputs = [text_input], outputs = [outputs])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU configuration for model training ( run only system have good GPU Card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Inputs (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_6 (KerasLayer)     {'input_type_ids':   0           ['Inputs[0][0]']                 \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_7 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer_6[0][0]',          \n",
      "                                None, 768),                       'keras_layer_6[0][1]',          \n",
      "                                 'sequence_output':               'keras_layer_6[0][2]']          \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " Dropout (Dropout)              (None, 768)          0           ['keras_layer_7[0][13]']         \n",
      "                                                                                                  \n",
      " Dense (Dense)                  (None, 1)            769         ['Dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model Summary to print\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model Compile with checkpoint callback and respective result metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model Compiling\n",
    "\n",
    "Metrics = [tf.keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
    "           tf.keras.metrics.Precision(name = 'precision'),\n",
    "           tf.keras.metrics.Recall(name = 'recall')\n",
    "           ]\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"/profane_Model CheckPoints\",\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "# compiling our model\n",
    "model.compile(optimizer ='adam',\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = Metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4148/4148 [==============================] - 2422s 584ms/step - loss: 0.2746 - accuracy: 0.8888 - precision: 0.8297 - recall: 0.5544 - val_loss: 0.2313 - val_accuracy: 0.9109 - val_precision: 0.8119 - val_recall: 0.7249\n",
      "Epoch 2/10\n",
      "4148/4148 [==============================] - 2418s 583ms/step - loss: 0.2385 - accuracy: 0.9054 - precision: 0.8197 - recall: 0.6714 - val_loss: 0.2291 - val_accuracy: 0.9078 - val_precision: 0.8906 - val_recall: 0.6173\n",
      "Epoch 3/10\n",
      "4148/4148 [==============================] - 2408s 581ms/step - loss: 0.2335 - accuracy: 0.9081 - precision: 0.8232 - recall: 0.6842 - val_loss: 0.2249 - val_accuracy: 0.9094 - val_precision: 0.8784 - val_recall: 0.6379\n",
      "Epoch 4/10\n",
      "4148/4148 [==============================] - 2406s 580ms/step - loss: 0.2331 - accuracy: 0.9079 - precision: 0.8197 - recall: 0.6876 - val_loss: 0.2163 - val_accuracy: 0.9138 - val_precision: 0.8702 - val_recall: 0.6713\n",
      "Epoch 5/10\n",
      "4148/4148 [==============================] - 2407s 580ms/step - loss: 0.2308 - accuracy: 0.9091 - precision: 0.8241 - recall: 0.6896 - val_loss: 0.2125 - val_accuracy: 0.9163 - val_precision: 0.8593 - val_recall: 0.6982\n",
      "Epoch 6/10\n",
      "4148/4148 [==============================] - 2406s 580ms/step - loss: 0.2313 - accuracy: 0.9094 - precision: 0.8230 - recall: 0.6928 - val_loss: 0.2131 - val_accuracy: 0.9175 - val_precision: 0.8296 - val_recall: 0.7421\n",
      "Epoch 7/10\n",
      "4148/4148 [==============================] - 2405s 580ms/step - loss: 0.2309 - accuracy: 0.9085 - precision: 0.8208 - recall: 0.6901 - val_loss: 0.2112 - val_accuracy: 0.9159 - val_precision: 0.8350 - val_recall: 0.7249\n",
      "Epoch 8/10\n",
      "4148/4148 [==============================] - 2412s 582ms/step - loss: 0.2287 - accuracy: 0.9098 - precision: 0.8240 - recall: 0.6947 - val_loss: 0.2102 - val_accuracy: 0.9162 - val_precision: 0.8519 - val_recall: 0.7060\n",
      "Epoch 9/10\n",
      "4148/4148 [==============================] - 2415s 582ms/step - loss: 0.2290 - accuracy: 0.9096 - precision: 0.8224 - recall: 0.6953 - val_loss: 0.2118 - val_accuracy: 0.9168 - val_precision: 0.8502 - val_recall: 0.7117\n",
      "Epoch 10/10\n",
      "4148/4148 [==============================] - 2418s 583ms/step - loss: 0.2291 - accuracy: 0.9097 - precision: 0.8223 - recall: 0.6958 - val_loss: 0.2178 - val_accuracy: 0.9127 - val_precision: 0.8958 - val_recall: 0.6403\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set[0], train_set[1],batch_size=32,validation_data=val_set,shuffle=True, epochs = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Inputs with unsupported characters which will be renamed to inputs in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/profane_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/profane_model\\assets\n"
     ]
    }
   ],
   "source": [
    "opt = tf.saved_model.SaveOptions(\n",
    "    namespace_whitelist=None,\n",
    "    save_debug_info=False,\n",
    "    function_aliases=None,\n",
    "    experimental_io_device=\"CPU:0\",\n",
    "    experimental_variable_policy=None,\n",
    "    experimental_custom_gradients=True\n",
    ")\n",
    "model.save(\"models/profane_model\",options=opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Save model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_weight/profane_model_weight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1153/1153 [==============================] - 600s 521ms/step - loss: 0.2179 - accuracy: 0.9124 - precision: 0.9035 - recall: 0.6377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21792438626289368,\n",
       " 0.9124243855476379,\n",
       " 0.9035120606422424,\n",
       " 0.6377449035644531]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set[0],test_set[1],batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evalution  Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1153/1153 [==============================] - 314s 273ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pred = model.predict(test_set[0])\n",
    "pred_int = tf.cast(pred, tf.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codej\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.79650674, 0.        ]),\n",
       " array([1., 0.]),\n",
       " array([0.88672836, 0.        ]),\n",
       " array([29368,  7503], dtype=int64))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_set[1],pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_GPU_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
