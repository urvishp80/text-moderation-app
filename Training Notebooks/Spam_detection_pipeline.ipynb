{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\codej\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Function\n",
    "\n",
    "def convert_to_number(series):\n",
    "    for i  in range(len(series)):\n",
    "        try:\n",
    "            series[i] = int(series[i])\n",
    "        except ValueError:\n",
    "            series[i] = None\n",
    "    return series\n",
    "\n",
    "\n",
    "def remove_html_tags_special_character(col: pd.Series) -> pd.Series:\n",
    "    tags_list = ['<p>' ,'</p>' , '<p*>',\n",
    "                 '<ul>','</ul>',\n",
    "                 '<li>','</li>',\n",
    "                 '<br>',\n",
    "                 '<strong>','</strong>',\n",
    "                 '<span*>','</span>',\n",
    "                 '<a href*>','</a>',\n",
    "                 '<em>','</em>','<br>','<br />','<div>','</div>','\\\\n','~']\n",
    "    for tag in tags_list:\n",
    "        col.replace(to_replace=tag,value='',regex=False,inplace=True)\n",
    "    return col\n",
    "\n",
    "punctuations_list = string.punctuation\n",
    "def remove_punctuations(text):\n",
    "    temp = str.maketrans('', '', punctuations_list)\n",
    "    text = str(text)\n",
    "    return text.translate(temp)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    "        word = word.lower()\n",
    " \n",
    "        if (word not in stop_words) and 'br' not in word:\n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output\n",
    "\n",
    "def balance_data(df,y_column_name):\n",
    "    ham_msg = df[df[y_column_name] == 0]\n",
    "    spam_msg = df[df[y_column_name] == 1]\n",
    "    print(ham_msg.shape)\n",
    "    print(spam_msg.shape)\n",
    "\n",
    "    if len(ham_msg) >= len(spam_msg):\n",
    "        ham_msg.sample(n=len(spam_msg),random_state=42)\n",
    "    else:\n",
    "        spam_msg.sample(n=len(ham_msg),random_state=42)\n",
    "    return pd.concat([ham_msg, spam_msg],ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,X_column_name=\"C3\",y_column_name=\"C5\",split_ratio=[0.1,0.2]):\n",
    "    \"\"\"\n",
    "    Preprocess the data for model training\n",
    "    Arguments:\n",
    "    df: Dataframe of raw data\n",
    "    X_column_name: represent the input x column name in DataFrame\n",
    "    Y_Column_nameL represent the target y column name in DataFrame\n",
    "    split_ratio: use to define spliting ratio for training and testing data default is 0.2 (20% of data is use for testing and 80% for training)\n",
    "    \"\"\"\n",
    "    # Target value preprocessing\n",
    "    df = df[[X_column_name,y_column_name]] # C3 for input column ,C5 target column\n",
    "    df[y_column_name] = pd.Series(convert_to_number(df[y_column_name].to_list()),name=y_column_name)\n",
    "    df = df[ df[y_column_name] <= 1]\n",
    "\n",
    "    #input Value preprocessing\n",
    "    #Step 1 remove html Tags an extra special character\n",
    "    df[X_column_name] = remove_html_tags_special_character(df[X_column_name])\n",
    "    df[X_column_name].replace(to_replace='\\n',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].replace(to_replace='\\\\?',value='',inplace=True,regex=True)\n",
    "    df[X_column_name].dropna(inplace=True)\n",
    "\n",
    "    # Balance Data\n",
    "    df = balance_data(df,y_column_name)\n",
    "    # Step 3 NLP Text Preprocessing\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda x: remove_punctuations(x))\n",
    "    df[X_column_name] = df[X_column_name].apply(lambda text: remove_stopwords(text))\n",
    "    \n",
    "    if len(split_ratio) == 2:\n",
    "        train_x, test_x, train_y, test_y = train_test_split(df[X_column_name],df[y_column_name],test_size=split_ratio[1])\n",
    "        train_x, val_x, train_y, val_y = train_test_split(train_x,train_y,test_size=split_ratio[0])\n",
    "        return ((train_x,train_y), (val_x, val_y), (test_x, test_y))\n",
    "    else:\n",
    "        train_x, test_x, train_y, test_y = train_test_split(df[X_column_name],df[y_column_name],test_size=split_ratio[0])\n",
    "        return ((train_x, train_y), (test_x, test_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Raw dataset File name for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codej\\AppData\\Local\\Temp\\ipykernel_26396\\2418128580.py:5: DtypeWarning: Columns (9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_df = pd.read_csv(raw_file, encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(Max_columns)])\n",
      "C:\\Users\\codej\\AppData\\Local\\Temp\\ipykernel_26396\\3843300502.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[y_column_name] = pd.Series(convert_to_number(df[y_column_name].to_list()),name=y_column_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578489, 2)\n",
      "(91143, 2)\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing raw Data\n",
    "DATASET_PATH = r\"A:\\CJ_Personal\\Upwork\\Text Moderation\\Dataset\\messages.csv\"\n",
    "INPUT_COLUMN_NAME = \"C3\"\n",
    "TARGET_COLUMN_NAME = \"C5\"\n",
    "Max_COLUMN = 100\n",
    "df = pd.read_csv(DATASET_PATH,encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(Max_COLUMN)])\n",
    "VALIDATION_RATIO = 0.1\n",
    "TESTING_RATIO = 0.2\n",
    "raw_df = pd.read_csv(DATASET_PATH, encoding=\"UTF-8\",names=[f\"C{i}\" for i in range(Max_COLUMN)])\n",
    "train_set, val_set, test_set = preprocess(raw_df,\n",
    "                                          INPUT_COLUMN_NAME,\n",
    "                                          TARGET_COLUMN_NAME,\n",
    "                                          [VALIDATION_RATIO,TESTING_RATIO])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text to Number\n",
    "### Plase Set the max length for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Text Data into Number\n",
    "max_len = 150\n",
    "train_tokenizer = Tokenizer()\n",
    "train_tokenizer.fit_on_texts(train_set[0])\n",
    "\n",
    "train_sequence = train_tokenizer.texts_to_sequences(train_set[0])\n",
    "val_sequence = train_tokenizer.texts_to_sequences(val_set[0])\n",
    "test_sequence = train_tokenizer.texts_to_sequences(test_set[0])\n",
    "\n",
    "train_sequence = pad_sequences(train_sequence,\n",
    "                                maxlen=max_len,\n",
    "                                padding='post',\n",
    "                                truncating='post')\n",
    "\n",
    "\n",
    "val_sequence = pad_sequences(val_sequence,\n",
    "                                maxlen=max_len,\n",
    "                                padding='post',\n",
    "                                truncating='post')\n",
    "\n",
    "\n",
    "test_sequence = pad_sequences(test_sequence,\n",
    "                                maxlen=max_len,\n",
    "                                padding='post',\n",
    "                                truncating='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is ready for Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection ( run the respective cells for model traning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning Model SVM ( Support vector machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "model.fit(train_sequence,train_set[1])\n",
    "\n",
    "prediction = model.predict(test_sequence)\n",
    "\n",
    "print(accuracy_score(test_set[1],prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                                    output_dim=64,\n",
    "                                    input_length=max_len))\n",
    "model.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'],\n",
    "              optimizer = 'SGD')\n",
    "\n",
    "es = EarlyStopping(patience=3,\n",
    "                   monitor = 'val_accuracy',\n",
    "                   restore_best_weights = True)\n",
    " \n",
    "lr = ReduceLROnPlateau(patience = 3,\n",
    "                       monitor = 'val_loss',\n",
    "                       factor = 0.8,\n",
    "                       verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_sequence, train_set[1],\n",
    "                    validation_data=(val_sequence, val_set[1]),\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    callbacks = [lr, es]\n",
    "                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Pre-build Bert Model Layer\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Model Wrapper ( Input and output layer Added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert model wth input and output layer wrapper\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape = (), dtype = tf.string, name = 'Inputs')\n",
    "preprocessed_text = bert_preprocessor(text_input)\n",
    "embeed = bert_encoder(preprocessed_text)\n",
    "dropout = tf.keras.layers.Dropout(0.1, name = 'Dropout')(embeed['pooled_output'])\n",
    "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'Dense')(dropout)\n",
    "# creating final model\n",
    "model = tf.keras.Model(inputs = [text_input], outputs = [outputs])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Configuration for model training (Run Only System have supported GPU Card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summry to show trainable and non-trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Inputs (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'input_mask': (Non  0           ['Inputs[0][0]']                 \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_2 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer_1[0][0]',          \n",
      "                                None, 768),                       'keras_layer_1[0][1]',          \n",
      "                                 'sequence_output':               'keras_layer_1[0][2]']          \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                768)}                                                             \n",
      "                                                                                                  \n",
      " Dropout (Dropout)              (None, 768)          0           ['keras_layer_2[0][13]']         \n",
      "                                                                                                  \n",
      " Dense (Dense)                  (None, 1)            769         ['Dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Compiling with Call backs and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model Compiling\n",
    "\n",
    "Metrics = [tf.keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
    "           tf.keras.metrics.Precision(name = 'precision'),\n",
    "           tf.keras.metrics.Recall(name = 'recall')\n",
    "           ]\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"/Model CheckPoints\",\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "# compiling our model\n",
    "model.compile(optimizer ='adam',\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = Metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3767/3767 [==============================] - 8717s 2s/step - loss: 0.4010 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3991 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3767/3767 [==============================] - 12433s 3s/step - loss: 0.3987 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4012 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3767/3767 [==============================] - 10346s 3s/step - loss: 0.3981 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4049 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3767/3767 [==============================] - 6668s 2s/step - loss: 0.3982 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3992 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3767/3767 [==============================] - 6672s 2s/step - loss: 0.3979 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3767/3767 [==============================] - 6684s 2s/step - loss: 0.3979 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3988 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3767/3767 [==============================] - 6689s 2s/step - loss: 0.3980 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3980 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3767/3767 [==============================] - 6687s 2s/step - loss: 0.3980 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.3989 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3767/3767 [==============================] - 6676s 2s/step - loss: 0.3981 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3767/3767 [==============================] - 6668s 2s/step - loss: 0.3978 - accuracy: 0.8641 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4009 - val_accuracy: 0.8627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set[0], train_set[1],batch_size=128,validation_data=val_set,shuffle=True, epochs = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Model Save ( Large File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Inputs with unsupported characters which will be renamed to inputs in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/spam_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/spam_model\\assets\n"
     ]
    }
   ],
   "source": [
    "opt = tf.saved_model.SaveOptions(\n",
    "    namespace_whitelist=None,\n",
    "    save_debug_info=False,\n",
    "    function_aliases=None,\n",
    "    experimental_io_device=\"CPU:0\",\n",
    "    experimental_variable_policy=None,\n",
    "    experimental_custom_gradients=True\n",
    ")\n",
    "model.save(\"models/spam_model\",options=opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model Weights Only ( lite weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"spam_model_weight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4186/4186 [==============================] - 1161s 277ms/step - loss: 0.3994 - accuracy: 0.8637 - precision: 0.0000e+00 - recall: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39939022064208984, 0.8637391924858093, 0.0, 0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set[0],test_set[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Classificatio Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4186/4186 [==============================] - 1147s 274ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "pred = model.predict(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codej\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.8637392, 0.       ]),\n",
       " array([1., 0.]),\n",
       " array([0.92688848, 0.        ]),\n",
       " array([115678,  18249], dtype=int64))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_int = tf.cast(pred, tf.int32)\n",
    "precision_recall_fscore_support(test_set[1],pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
